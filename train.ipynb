{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque\n",
    "from random import randint, random, sample\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from src.deep_q_network import DeepQNetwork\n",
    "from src.tetris import Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.width = 10\n",
    "        self.height = 20\n",
    "        self.block_size = 30\n",
    "        self.batch_size = 512\n",
    "        self.lr = 1e-3\n",
    "        self.gamma = 0.99\n",
    "        self.initial_epsilon = 1\n",
    "        self.final_epsilon = 1e-3\n",
    "        self.num_decay_epochs = 2000\n",
    "        self.num_epochs = 3000\n",
    "        self.save_interval = 1000\n",
    "        self.replay_memory_size = 30000\n",
    "        self.log_path = \"tensorboard\"\n",
    "        self.saved_path = \"trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "total_episodes = 0\n",
    "max_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "else:\n",
    "    torch.manual_seed(123)\n",
    "if os.path.isdir(opt.log_path):\n",
    "    shutil.rmtree(opt.log_path)\n",
    "os.makedirs(opt.log_path)\n",
    "writer = SummaryWriter(opt.log_path)\n",
    "env = Tetris(width=opt.width, height=opt.height, block_size=opt.block_size)\n",
    "model = DeepQNetwork()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    state = state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=opt.replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m next_state \u001b[39m=\u001b[39m next_states[index, :]\n\u001b[0;32m     23\u001b[0m action \u001b[39m=\u001b[39m next_actions[index]\n\u001b[1;32m---> 25\u001b[0m reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m     28\u001b[0m     next_state \u001b[39m=\u001b[39m next_state\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\I539052\\Desktop\\GitHub\\Tetris-deep-Q-learning-pytorch\\src\\tetris.py:213\u001b[0m, in \u001b[0;36mTetris.step\u001b[1;34m(self, action, render, video)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_pos[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m render:\n\u001b[1;32m--> 213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender(video)\n\u001b[0;32m    215\u001b[0m overflow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtruncate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpiece, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_pos)\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m overflow:\n",
      "File \u001b[1;32mc:\\Users\\I539052\\Desktop\\GitHub\\Tetris-deep-Q-learning-pytorch\\src\\tetris.py:272\u001b[0m, in \u001b[0;36mTetris.render\u001b[1;34m(self, video)\u001b[0m\n\u001b[0;32m    269\u001b[0m     video\u001b[39m.\u001b[39mwrite(img)\n\u001b[0;32m    271\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mDeep Q-Learning Tetris\u001b[39m\u001b[39m\"\u001b[39m, img)\n\u001b[1;32m--> 272\u001b[0m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while epoch < opt.num_epochs:\n",
    "    next_steps = env.get_next_states()\n",
    "    # Exploration or exploitation\n",
    "    epsilon = opt.final_epsilon + (max(opt.num_decay_epochs - epoch, 0) * (\n",
    "            opt.initial_epsilon - opt.final_epsilon) / opt.num_decay_epochs)\n",
    "    u = random()\n",
    "    random_action = u <= epsilon\n",
    "    next_actions, next_states = zip(*next_steps.items())\n",
    "    next_states = torch.stack(next_states)\n",
    "    if torch.cuda.is_available():\n",
    "        next_states = next_states.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(next_states)[:, 0]\n",
    "    model.train()\n",
    "    if random_action:\n",
    "        index = randint(0, len(next_steps) - 1)\n",
    "    else:\n",
    "        index = torch.argmax(predictions).item()\n",
    "\n",
    "    next_state = next_states[index, :]\n",
    "    action = next_actions[index]\n",
    "\n",
    "    reward, done = env.step(action, render=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        next_state = next_state.cuda()\n",
    "    replay_memory.append([state, reward, next_state, done])\n",
    "    if done:\n",
    "        final_score = env.score\n",
    "        final_tetrominoes = env.tetrominoes\n",
    "        final_cleared_lines = env.cleared_lines\n",
    "        state = env.reset()\n",
    "        if torch.cuda.is_available():\n",
    "            state = state.cuda()\n",
    "    else:\n",
    "        state = next_state\n",
    "        continue\n",
    "    if len(replay_memory) < opt.replay_memory_size / 10:\n",
    "        continue\n",
    "    total_reward += final_score\n",
    "    total_episodes += 1\n",
    "    average_reward = total_reward / total_episodes\n",
    "    print(\"Average reward: {}\".format(average_reward))\n",
    "    writer.add_scalar('Train/Average Reward', average_reward, epoch - 1)\n",
    "    max_score = max(max_score, final_score)\n",
    "    print(\"Maximal achieved score: {}\".format(max_score))\n",
    "    writer.add_scalar('Train/Max Score', max_score, epoch - 1)\n",
    "    epoch += 1\n",
    "    batch = sample(replay_memory, min(len(replay_memory), opt.batch_size))\n",
    "    state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "    state_batch = torch.stack(tuple(state for state in state_batch))\n",
    "    reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n",
    "    next_state_batch = torch.stack(tuple(state for state in next_state_batch))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        state_batch = state_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "\n",
    "    q_values = model(state_batch)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        next_prediction_batch = model(next_state_batch)\n",
    "    model.train()\n",
    "\n",
    "    y_batch = torch.cat(\n",
    "        tuple(reward if done else reward + opt.gamma * prediction for reward, done, prediction in\n",
    "                zip(reward_batch, done_batch, next_prediction_batch)))[:, None]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(q_values, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {}/{}, Action: {}, Score: {}, Tetrominoes {}, Cleared lines: {}\".format(\n",
    "        epoch,\n",
    "        opt.num_epochs,\n",
    "        action,\n",
    "        final_score,\n",
    "        final_tetrominoes,\n",
    "        final_cleared_lines))\n",
    "    writer.add_scalar('Train/Score', final_score, epoch - 1)\n",
    "    writer.add_scalar('Train/Tetrominoes', final_tetrominoes, epoch - 1)\n",
    "    writer.add_scalar('Train/Cleared lines', final_cleared_lines, epoch - 1)\n",
    "    writer.add_scalar('Train/Loss', loss.item(), epoch - 1)\n",
    "    writer.add_scalar('Train/Epsilon', epsilon, epoch - 1)\n",
    "\n",
    "    if epoch > 0 and epoch % opt.save_interval == 0:\n",
    "        torch.save(model, \"{}/tetris_{}\".format(opt.saved_path, epoch))\n",
    "\n",
    "torch.save(model, \"{}/tetris\".format(opt.saved_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
